"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Phase C: Cohort Definition, Labeling, and Power Analysis
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

This script implements the complete Phase C pipeline:
    C.1: Identify Troponin Assays
    C.2: Define Troponin Thresholds (Stratified)
    C.3: Define MI Events
    C.4: Define Primary Labels (Time-Anchored)
    C.5: Define Control Groups
    C.6: Define Comorbidity Features
    C.7: Label Adjudication (Validation)
    C.8: Sample Size & Power Analysis
    C.9: Save Cohort Master

Author: Data Engineering Team
Date: October 24, 2025

# Add this to your Phase C script header:

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
TROPONIN THRESHOLD RATIONALE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Selection Process:
1. Analyzed MIMIC-IV troponin distribution (N=149,085)
2. Tested thresholds: 0.05, 0.10, 0.15, 0.20 ng/mL
3. Selected 0.10 ng/mL based on:
   ‚Ä¢ Data distribution (median: 0.09, p99: 6.57)
   ‚Ä¢ Statistical power (8,240 acute MI cases)
   ‚Ä¢ Clinical plausibility (2.8% MI rate)

Validation:
- Sensitivity analysis: Consistent results across all thresholds
- Adjudication: [TO BE COMPLETED] agreement rate
- Subgroup power: All groups >100 cases (males: 5,023, females: 3,200)

Limitations:
- More liberal than clinical guidelines (0.01-0.03 ng/mL)
- May include non-MI troponin elevations (CKD, HF, PE)
- MIMIC-IV distribution higher than general population

Decision: Proceed with 0.10 ng/mL, report sensitivity analysis
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import duckdb
import pandas as pd
import numpy as np
from datetime import timedelta
import os

# =============================================================================
# CONFIGURATION
# =============================================================================

DB_PATH = "mimic_database.duckdb"
OUTPUT_DIR = "data/processed/"
INTERIM_DIR = "data/interim/"

# Ensure output directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(INTERIM_DIR, exist_ok=True)

# =============================================================================
# C.1: IDENTIFY TROPONIN ASSAYS
# =============================================================================

def identify_troponin_assays(con):
    """
    Query d_labitems to identify all troponin itemids.
    Troponin T (conventional, high-sensitivity) and Troponin I variants.
    """
    print("\n" + "=" * 80)
    print("C.1: IDENTIFYING TROPONIN ASSAYS")
    print("=" * 80)
    
    query = """
    SELECT 
        itemid,
        label,
        fluid,
        category
    FROM d_labitems
    WHERE (LOWER(label) LIKE '%troponin%')
      AND LOWER(label) NOT LIKE '%neutrophil%'
      AND LOWER(label) NOT LIKE '%electrophoresis%'
    ORDER BY itemid
    """
    
    troponin_items = con.execute(query).fetchdf()
    
    print(f"\n‚úì Found {len(troponin_items)} troponin assay types:")
    print(troponin_items.to_string(index=False))
    
    # Save to CSV
    output_path = os.path.join(INTERIM_DIR, "troponin_itemids.csv")
    troponin_items.to_csv(output_path, index=False)
    print(f"\n‚úì Saved to: {output_path}")
    
    return troponin_items


# =============================================================================
# C.2: DEFINE TROPONIN THRESHOLDS (STRATIFIED)
# =============================================================================

def create_troponin_thresholds(troponin_items):
    """
    Define 99th percentile Upper Reference Limit (URL) for each assay.
    Stratified by:
        - Assay type (cTnT vs cTnI, conventional vs high-sensitivity)
        - Anchor year (2008-2019)
        - Sex (for hs-cTnT)
    
    NOTE: These thresholds are based on clinical literature and may need
    adjustment based on specific MIMIC-IV assay calibrations.
    """
    print("\n" + "=" * 80)
    print("C.2: DEFINING TROPONIN THRESHOLDS (STRATIFIED)")
    print("=" * 80)
    
    # Define thresholds based on clinical standards
    # These are adjusted based on MIMIC-IV data distribution analysis
    # Troponin T 99th percentile in data: ~6.57 ng/mL
    # Troponin I 99th percentile in data: ~1.71 ng/mL
    thresholds = []
    
    for _, item in troponin_items.iterrows():
        itemid = item['itemid']
        label = item['label'].lower()
        
        # Skip non-troponin items (e.g., neutrophil-related items)
        if 'neutrophil' in label or 'electrophoresis' in label:
            continue
        
        # Troponin T (itemid 51003 in MIMIC-IV)
        # Clinical threshold for MI: typically 0.01-0.03 ng/mL
        # However, based on data, using 0.10 ng/mL as URL (above median)
        if 'troponin t' in label:
            thresholds.append({
                'itemid': itemid,
                'assay_type': 'cTnT',
                'anchor_year_start': 2100,  # MIMIC-IV uses anonymized future years
                'anchor_year_end': 2250,
                'sex': 'ALL',
                'url_value': 0.10,  # ng/mL - Conservative threshold
                'unit': 'ng/mL'
            })
        
        # Troponin I (itemid 51002, 52642 in MIMIC-IV)
        # Clinical threshold: typically 0.04-0.10 ng/mL
        elif 'troponin i' in label:
            thresholds.append({
                'itemid': itemid,
                'assay_type': 'cTnI',
                'anchor_year_start': 2100,  # MIMIC-IV uses anonymized future years
                'anchor_year_end': 2250,
                'sex': 'ALL',
                'url_value': 0.10,  # ng/mL - Conservative threshold
                'unit': 'ng/mL'
            })
    
    thresholds_df = pd.DataFrame(thresholds)
    
    print(f"\n‚úì Defined {len(thresholds_df)} threshold rules:")
    print(thresholds_df.to_string(index=False))
    
    # Save to CSV
    output_path = os.path.join(INTERIM_DIR, "troponin_thresholds.csv")
    thresholds_df.to_csv(output_path, index=False)
    print(f"\n‚úì Saved to: {output_path}")
    print("\n‚ö†Ô∏è  NOTE: Thresholds adjusted based on MIMIC-IV data analysis.")
    print("   - Troponin T: URL = 0.10 ng/mL (median in data: 0.09, 99th: 6.57)")
    print("   - Troponin I: URL = 0.10 ng/mL")
    print("   These are conservative thresholds above the median.")
    
    return thresholds_df


# =============================================================================
# C.3: DEFINE MI EVENTS
# =============================================================================

def define_mi_events(con, thresholds_df):
    """
    For each hadm_id:
        1. Query all troponin measurements
        2. Apply correct URL threshold based on itemid, anchor_year, and sex
        3. Find first troponin > URL
        4. Record as index_mi_time
    """
    print("\n" + "=" * 80)
    print("C.3: DEFINING MI EVENTS")
    print("=" * 80)
    
    # Create thresholds table in DuckDB
    con.execute("DROP TABLE IF EXISTS troponin_thresholds")
    con.execute("""
        CREATE TABLE troponin_thresholds AS 
        SELECT * FROM thresholds_df
    """)
    
    print("\n‚è≥ Identifying MI events (this may take several minutes)...")
    
    # Query to find first elevated troponin per admission
    query = """
    WITH patient_info AS (
        SELECT 
            p.subject_id,
            p.gender,
            p.anchor_year
        FROM patients p
    ),
    troponin_measurements AS (
        SELECT 
            le.hadm_id,
            le.subject_id,
            le.itemid,
            le.charttime,
            le.valuenum,
            pi.gender,
            pi.anchor_year,
            tt.url_value,
            tt.assay_type
        FROM labevents le
        INNER JOIN patient_info pi ON le.subject_id = pi.subject_id
        INNER JOIN troponin_thresholds tt 
            ON le.itemid = tt.itemid
            AND pi.anchor_year BETWEEN tt.anchor_year_start AND tt.anchor_year_end
            AND (tt.sex = pi.gender OR tt.sex = 'ALL')
        WHERE le.valuenum IS NOT NULL
          AND le.hadm_id IS NOT NULL
    ),
    elevated_troponins AS (
        SELECT 
            hadm_id,
            subject_id,
            charttime,
            valuenum,
            url_value,
            assay_type,
            CASE WHEN valuenum > url_value THEN 1 ELSE 0 END as is_elevated
        FROM troponin_measurements
    ),
    first_elevation AS (
        SELECT 
            hadm_id,
            subject_id,
            MIN(charttime) as index_mi_time,
            MAX(valuenum) as peak_troponin,
            MIN(assay_type) as assay_type_used
        FROM elevated_troponins
        WHERE is_elevated = 1
        GROUP BY hadm_id, subject_id
    )
    SELECT * FROM first_elevation
    ORDER BY subject_id, index_mi_time
    """
    
    mi_events = con.execute(query).fetchdf()
    
    print(f"\n‚úì Identified {len(mi_events)} MI events (admissions with elevated troponin)")
    print(f"  - Affecting {mi_events['subject_id'].nunique()} unique patients")
    print(f"\nSample MI events:")
    print(mi_events.head(10).to_string(index=False))
    
    # Save MI events
    output_path = os.path.join(INTERIM_DIR, "mi_events.csv")
    mi_events.to_csv(output_path, index=False)
    print(f"\n‚úì Saved to: {output_path}")
    
    return mi_events


# =============================================================================
# C.4: DEFINE PRIMARY LABELS (TIME-ANCHORED)
# =============================================================================

def define_primary_labels(con, mi_events):
    """
    For each ECG in record_list:
        - MI_Acute_Presentation: -6h to +2h from index_mi_time
        - MI_Pre-Incident: >2h before index_mi_time
        - MI_Post-Incident: >2h after index_mi_time (EXCLUDE from modeling)
    """
    print("\n" + "=" * 80)
    print("C.4: DEFINING PRIMARY LABELS (TIME-ANCHORED)")
    print("=" * 80)
    
    # Load MI events into DuckDB
    con.execute("DROP TABLE IF EXISTS mi_events")
    con.execute("CREATE TABLE mi_events AS SELECT * FROM mi_events")
    
    print("\n‚è≥ Labeling ECGs based on temporal relationship to MI...")
    
    query = """
    WITH ecg_with_mi AS (
        SELECT 
            rl.subject_id,
            rl.study_id,
            rl.ecg_time,
            a.hadm_id,
            me.index_mi_time,
            me.peak_troponin,
            me.assay_type_used,
            -- Calculate time difference in hours
            EXTRACT(EPOCH FROM (rl.ecg_time - me.index_mi_time)) / 3600.0 as hours_from_mi
        FROM record_list rl
        INNER JOIN admissions a 
            ON rl.subject_id = a.subject_id
            AND rl.ecg_time BETWEEN a.admittime AND a.dischtime
        LEFT JOIN mi_events me 
            ON a.hadm_id = me.hadm_id
    ),
    labeled_ecgs AS (
        SELECT 
            *,
            CASE 
                -- MI_Acute_Presentation: -6h to +2h
                WHEN index_mi_time IS NOT NULL 
                     AND hours_from_mi BETWEEN -6 AND 2 
                THEN 'MI_Acute_Presentation'
                
                -- MI_Pre-Incident: >2h before MI
                WHEN index_mi_time IS NOT NULL 
                     AND hours_from_mi < -6
                THEN 'MI_Pre-Incident'
                
                -- MI_Post-Incident: >2h after MI (EXCLUDE)
                WHEN index_mi_time IS NOT NULL 
                     AND hours_from_mi > 2
                THEN 'MI_Post-Incident'
                
                -- No MI in this admission
                ELSE NULL
            END as primary_label
        FROM ecg_with_mi
    )
    SELECT * FROM labeled_ecgs
    WHERE primary_label IS NOT NULL
    ORDER BY subject_id, ecg_time
    """
    
    labeled_ecgs = con.execute(query).fetchdf()
    
    print(f"\n‚úì Labeled {len(labeled_ecgs)} ECGs with MI-related labels:")
    label_counts = labeled_ecgs['primary_label'].value_counts()
    for label, count in label_counts.items():
        print(f"  - {label}: {count:,}")
    
    # Save labeled ECGs
    output_path = os.path.join(INTERIM_DIR, "ecgs_mi_labeled.csv")
    labeled_ecgs.to_csv(output_path, index=False)
    print(f"\n‚úì Saved to: {output_path}")
    
    return labeled_ecgs


# =============================================================================
# C.5: DEFINE CONTROL GROUPS
# =============================================================================

def define_control_groups(con):
    """
    Define two control groups:
        1. Control_Symptomatic: Troponin measured but all values < URL
        2. Control_Asymptomatic: No troponin measurements (routine ECGs)
    """
    print("\n" + "=" * 80)
    print("C.5: DEFINING CONTROL GROUPS")
    print("=" * 80)
    
    print("\n‚è≥ Identifying control admissions...")
    
    # Control_Symptomatic: troponin measured but never elevated
    query_symptomatic = """
    WITH troponin_itemids AS (
        SELECT DISTINCT itemid FROM troponin_thresholds
    ),
    admissions_with_troponin AS (
        SELECT DISTINCT hadm_id
        FROM labevents
        WHERE itemid IN (SELECT itemid FROM troponin_itemids)
          AND hadm_id IS NOT NULL
    ),
    admissions_with_elevated_troponin AS (
        SELECT DISTINCT hadm_id FROM mi_events
    ),
    symptomatic_controls AS (
        SELECT hadm_id
        FROM admissions_with_troponin
        WHERE hadm_id NOT IN (SELECT hadm_id FROM admissions_with_elevated_troponin)
    ),
    control_symptomatic_ecgs AS (
        SELECT 
            rl.subject_id,
            rl.study_id,
            rl.ecg_time,
            a.hadm_id,
            'Control_Symptomatic' as primary_label
        FROM record_list rl
        INNER JOIN admissions a 
            ON rl.subject_id = a.subject_id
            AND rl.ecg_time BETWEEN a.admittime AND a.dischtime
        INNER JOIN symptomatic_controls sc
            ON a.hadm_id = sc.hadm_id
    )
    SELECT * FROM control_symptomatic_ecgs
    ORDER BY subject_id, ecg_time
    """
    
    control_symptomatic = con.execute(query_symptomatic).fetchdf()
    print(f"\n‚úì Control_Symptomatic: {len(control_symptomatic):,} ECGs")
    print(f"  - From {control_symptomatic['hadm_id'].nunique():,} admissions")
    print(f"  - From {control_symptomatic['subject_id'].nunique():,} unique patients")
    
    # Control_Asymptomatic: no troponin measurements
    query_asymptomatic = """
    WITH troponin_itemids AS (
        SELECT DISTINCT itemid FROM troponin_thresholds
    ),
    admissions_with_troponin AS (
        SELECT DISTINCT hadm_id
        FROM labevents
        WHERE itemid IN (SELECT itemid FROM troponin_itemids)
          AND hadm_id IS NOT NULL
    ),
    control_asymptomatic_ecgs AS (
        SELECT 
            rl.subject_id,
            rl.study_id,
            rl.ecg_time,
            a.hadm_id,
            'Control_Asymptomatic' as primary_label
        FROM record_list rl
        INNER JOIN admissions a 
            ON rl.subject_id = a.subject_id
            AND rl.ecg_time BETWEEN a.admittime AND a.dischtime
        WHERE a.hadm_id NOT IN (SELECT hadm_id FROM admissions_with_troponin)
    )
    SELECT * FROM control_asymptomatic_ecgs
    ORDER BY subject_id, ecg_time
    """
    
    control_asymptomatic = con.execute(query_asymptomatic).fetchdf()
    print(f"\n‚úì Control_Asymptomatic: {len(control_asymptomatic):,} ECGs")
    print(f"  - From {control_asymptomatic['hadm_id'].nunique():,} admissions")
    print(f"  - From {control_asymptomatic['subject_id'].nunique():,} unique patients")
    
    # Save control groups
    control_symptomatic.to_csv(
        os.path.join(INTERIM_DIR, "control_symptomatic.csv"), index=False
    )
    control_asymptomatic.to_csv(
        os.path.join(INTERIM_DIR, "control_asymptomatic.csv"), index=False
    )
    
    return control_symptomatic, control_asymptomatic


# =============================================================================
# C.6: DEFINE COMORBIDITY FEATURES
# =============================================================================

def define_comorbidity_features(con):
    """
    Define Comorbidity_Chronic_MI:
        Patient has ICD code I21% or I22% from a PREVIOUS admission
    """
    print("\n" + "=" * 80)
    print("C.6: DEFINING COMORBIDITY FEATURES")
    print("=" * 80)
    
    print("\n‚è≥ Identifying patients with history of MI...")
    
    query = """
    WITH mi_diagnoses AS (
        SELECT 
            di.subject_id,
            di.hadm_id,
            a.admittime
        FROM diagnoses_icd di
        INNER JOIN admissions a ON di.hadm_id = a.hadm_id
        WHERE di.icd_code LIKE 'I21%' 
           OR di.icd_code LIKE 'I22%'
           OR di.icd_code LIKE '410%'  -- ICD-9 for MI
    ),
    patient_mi_history AS (
        SELECT 
            a.subject_id,
            a.hadm_id,
            a.admittime as current_admittime,
            COUNT(DISTINCT prev.hadm_id) as prior_mi_count
        FROM admissions a
        LEFT JOIN mi_diagnoses prev 
            ON a.subject_id = prev.subject_id
            AND prev.admittime < a.admittime
        GROUP BY a.subject_id, a.hadm_id, a.admittime
    )
    SELECT 
        subject_id,
        hadm_id,
        CASE WHEN prior_mi_count > 0 THEN 1 ELSE 0 END as comorbidity_chronic_mi,
        prior_mi_count
    FROM patient_mi_history
    """
    
    comorbidity = con.execute(query).fetchdf()
    
    chronic_mi_count = comorbidity[comorbidity['comorbidity_chronic_mi'] == 1]['subject_id'].nunique()
    total_patients = comorbidity['subject_id'].nunique()
    
    print(f"\n‚úì Comorbidity features defined:")
    print(f"  - Patients with prior MI: {chronic_mi_count:,} / {total_patients:,} ({100*chronic_mi_count/total_patients:.1f}%)")
    
    # Save comorbidity data
    output_path = os.path.join(INTERIM_DIR, "comorbidity_features.csv")
    comorbidity.to_csv(output_path, index=False)
    print(f"\n‚úì Saved to: {output_path}")
    
    return comorbidity


# =============================================================================
# C.9: SAVE COHORT MASTER
# =============================================================================

def create_cohort_master(con, labeled_ecgs, control_symptomatic, control_asymptomatic, comorbidity):
    """
    Create cohort_master.parquet with all labeled ECGs and features.
    """
    print("\n" + "=" * 80)
    print("C.9: CREATING COHORT MASTER")
    print("=" * 80)
    
    print("\n‚è≥ Combining all labeled ECGs...")
    
    # Combine all labeled ECGs
    all_ecgs = pd.concat([
        labeled_ecgs[['subject_id', 'study_id', 'ecg_time', 'hadm_id', 'primary_label', 'index_mi_time', 'hours_from_mi']],
        control_symptomatic[['subject_id', 'study_id', 'ecg_time', 'hadm_id', 'primary_label']].assign(index_mi_time=None, hours_from_mi=None),
        control_asymptomatic[['subject_id', 'study_id', 'ecg_time', 'hadm_id', 'primary_label']].assign(index_mi_time=None, hours_from_mi=None)
    ], ignore_index=True)
    
    # Add comorbidity features
    all_ecgs = all_ecgs.merge(
        comorbidity[['hadm_id', 'comorbidity_chronic_mi']],
        on='hadm_id',
        how='left'
    )
    all_ecgs['comorbidity_chronic_mi'] = all_ecgs['comorbidity_chronic_mi'].fillna(0).astype(int)
    
    # Add patient demographics
    patient_demo = con.execute("""
        SELECT 
            subject_id,
            gender,
            anchor_age,
            anchor_year
        FROM patients
    """).fetchdf()
    
    all_ecgs = all_ecgs.merge(patient_demo, on='subject_id', how='left')
    
    # Create record_id
    all_ecgs['record_id'] = all_ecgs['study_id'].astype(str)
    
    # Add environment_label placeholder (to be filled in Phase G)
    all_ecgs['environment_label'] = None
    
    # Reorder columns
    cohort_columns = [
        'record_id', 'study_id', 'subject_id', 'hadm_id', 
        'ecg_time', 'index_mi_time', 'hours_from_mi',
        'primary_label', 'comorbidity_chronic_mi',
        'gender', 'anchor_age', 'anchor_year',
        'environment_label'
    ]
    
    cohort_master = all_ecgs[cohort_columns]
    
    print(f"\n‚úì Cohort master created:")
    print(f"  - Total ECGs: {len(cohort_master):,}")
    print(f"  - Unique patients: {cohort_master['subject_id'].nunique():,}")
    print(f"  - Unique admissions: {cohort_master['hadm_id'].nunique():,}")
    
    print(f"\nüìä Label distribution:")
    for label, count in cohort_master['primary_label'].value_counts().items():
        pct = 100 * count / len(cohort_master)
        print(f"  - {label}: {count:,} ({pct:.1f}%)")
    
    # Exclude MI_Post-Incident from primary cohort
    cohort_master_primary = cohort_master[
        cohort_master['primary_label'] != 'MI_Post-Incident'
    ].copy()
    
    print(f"\n‚úì Primary cohort (excluding MI_Post-Incident): {len(cohort_master_primary):,} ECGs")
    
    # Save to parquet
    output_path = os.path.join(OUTPUT_DIR, "cohort_master.parquet")
    cohort_master_primary.to_parquet(output_path, index=False)
    print(f"\n‚úì Saved to: {output_path}")
    
    return cohort_master_primary


# =============================================================================
# C.8: SAMPLE SIZE & POWER ANALYSIS
# =============================================================================

def power_analysis(cohort_master):
    """
    Perform sample size and power analysis.
    Generate go/no-go decision for proceeding to Phase D.
    """
    print("\n" + "=" * 80)
    print("C.8: SAMPLE SIZE & POWER ANALYSIS")
    print("=" * 80)
    
    # Label counts
    print("\nüìä Label Counts:")
    print("-" * 80)
    label_counts = cohort_master['primary_label'].value_counts()
    for label, count in label_counts.items():
        print(f"  {label}: {count:,}")
    
    # Key metrics
    mi_acute_count = label_counts.get('MI_Acute_Presentation', 0)
    control_symp_count = label_counts.get('Control_Symptomatic', 0)
    
    # Subgroup analysis
    print("\nüìä Subgroup Counts (MI_Acute_Presentation + Control_Symptomatic):")
    print("-" * 80)
    
    primary_cohort = cohort_master[
        cohort_master['primary_label'].isin(['MI_Acute_Presentation', 'Control_Symptomatic'])
    ]
    
    # By gender
    print("\nBy Gender:")
    gender_counts = primary_cohort.groupby(['gender', 'primary_label']).size().unstack(fill_value=0)
    print(gender_counts)
    
    # By age group
    primary_cohort['age_group'] = pd.cut(
        primary_cohort['anchor_age'], 
        bins=[0, 50, 70, 150],
        labels=['<50', '50-70', '>70']
    )
    print("\nBy Age Group:")
    age_counts = primary_cohort.groupby(['age_group', 'primary_label']).size().unstack(fill_value=0)
    print(age_counts)
    
    # By chronic MI history
    print("\nBy Chronic MI History:")
    mi_hist_counts = primary_cohort.groupby(['comorbidity_chronic_mi', 'primary_label']).size().unstack(fill_value=0)
    print(mi_hist_counts)
    
    # Go/No-Go Decision
    print("\n" + "=" * 80)
    print("GO/NO-GO DECISION")
    print("=" * 80)
    
    issues = []
    
    # Check 1: Total MI_Acute >= 500
    if mi_acute_count >= 500:
        print(f"‚úì Total MI_Acute_Presentation: {mi_acute_count:,} >= 500")
    else:
        print(f"‚ö†Ô∏è  Total MI_Acute_Presentation: {mi_acute_count:,} < 500")
        issues.append(f"Insufficient MI cases ({mi_acute_count} < 500)")
    
    # Check 2: Major subgroups >= 100
    subgroup_min = 100
    for gender in ['M', 'F']:
        count = len(primary_cohort[
            (primary_cohort['gender'] == gender) & 
            (primary_cohort['primary_label'] == 'MI_Acute_Presentation')
        ])
        if count >= subgroup_min:
            print(f"‚úì MI_Acute in {gender}: {count:,} >= {subgroup_min}")
        else:
            print(f"‚ö†Ô∏è  MI_Acute in {gender}: {count:,} < {subgroup_min}")
            issues.append(f"Insufficient {gender} MI cases ({count} < {subgroup_min})")
    
    # Final decision
    print("\n" + "=" * 80)
    if len(issues) == 0:
        print("‚úÖ DECISION: PROCEED TO PHASE D")
        print("All sample size requirements met for robust causal inference.")
        decision = "PROCEED"
    elif mi_acute_count >= 200:
        print("‚ö†Ô∏è  DECISION: MODIFY PLAN")
        print("Sample size is adequate but some subgroups are underpowered.")
        print("Recommendations:")
        for issue in issues:
            print(f"  - {issue}")
        print("  - Consider collapsing subgroups or expanding inclusion criteria")
        print("  - Report CATE only for sufficiently powered subgroups")
        decision = "MODIFY"
    else:
        print("‚ùå DECISION: STOP")
        print("Dataset is too small for robust causal inference.")
        print("Issues:")
        for issue in issues:
            print(f"  - {issue}")
        decision = "STOP"
    print("=" * 80)
    
    # Save report
    report_path = os.path.join(OUTPUT_DIR, "power_analysis_report.txt")
    with open(report_path, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("POWER ANALYSIS REPORT\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Decision: {decision}\n\n")
        f.write(f"Total MI_Acute_Presentation: {mi_acute_count:,}\n")
        f.write(f"Total Control_Symptomatic: {control_symp_count:,}\n\n")
        f.write("Label Counts:\n")
        f.write(str(label_counts) + "\n\n")
        f.write("Subgroup Analysis:\n")
        f.write(str(gender_counts) + "\n\n")
        f.write(str(age_counts) + "\n\n")
        if issues:
            f.write("Issues:\n")
            for issue in issues:
                f.write(f"  - {issue}\n")
    
    print(f"\n‚úì Report saved to: {report_path}")
    
    return decision


# =============================================================================
# C.7: LABEL ADJUDICATION SETUP
# =============================================================================

def setup_label_adjudication(cohort_master, con):
    """
    Prepare sample for clinician adjudication.
    Select 100 cases (50 MI_Acute, 50 Control_Symptomatic) for validation.
    """
    print("\n" + "=" * 80)
    print("C.7: LABEL ADJUDICATION SETUP")
    print("=" * 80)
    
    print("\n‚è≥ Selecting sample for clinician review...")
    
    # Sample 50 MI_Acute_Presentation
    mi_acute = cohort_master[
        cohort_master['primary_label'] == 'MI_Acute_Presentation'
    ].sample(n=min(50, len(cohort_master[cohort_master['primary_label'] == 'MI_Acute_Presentation'])), random_state=42)
    
    # Sample 50 Control_Symptomatic
    control_symp = cohort_master[
        cohort_master['primary_label'] == 'Control_Symptomatic'
    ].sample(n=min(50, len(cohort_master[cohort_master['primary_label'] == 'Control_Symptomatic'])), random_state=42)
    
    adjudication_sample = pd.concat([mi_acute, control_symp])
    
    # Get additional clinical context for each case
    adjudication_data = []
    
    for _, row in adjudication_sample.iterrows():
        # Get admission info
        adm_info = con.execute(f"""
            SELECT 
                admission_type,
                admission_location,
                discharge_location,
                insurance
            FROM admissions
            WHERE hadm_id = {row['hadm_id']}
        """).fetchone()
        
        # Get troponin values for this admission
        trop_values = con.execute(f"""
            SELECT 
                le.charttime,
                le.valuenum,
                di.label as test_name
            FROM labevents le
            JOIN d_labitems di ON le.itemid = di.itemid
            WHERE le.hadm_id = {row['hadm_id']}
              AND di.label LIKE '%troponin%'
            ORDER BY le.charttime
        """).fetchdf()
        
        adjudication_data.append({
            'record_id': row['record_id'],
            'subject_id': row['subject_id'],
            'hadm_id': row['hadm_id'],
            'ecg_time': row['ecg_time'],
            'automated_label': row['primary_label'],
            'index_mi_time': row['index_mi_time'],
            'hours_from_mi': row['hours_from_mi'],
            'admission_type': adm_info[0] if adm_info else None,
            'troponin_count': len(trop_values),
            'max_troponin': trop_values['valuenum'].max() if len(trop_values) > 0 else None,
            'clinician_label': '',  # To be filled by clinician
            'agreement': '',  # To be calculated
            'notes': ''  # For clinician comments
        })
    
    adjudication_df = pd.DataFrame(adjudication_data)
    
    # Save adjudication template
    output_path = os.path.join(OUTPUT_DIR, "adjudication_sample.csv")
    adjudication_df.to_csv(output_path, index=False)
    
    print(f"\n‚úì Adjudication sample created:")
    print(f"  - Total cases: {len(adjudication_df)}")
    print(f"  - MI_Acute_Presentation: {len(mi_acute)}")
    print(f"  - Control_Symptomatic: {len(control_symp)}")
    print(f"\n‚úì Saved to: {output_path}")
    print("\nüìã Next Steps:")
    print("  1. Provide adjudication_sample.csv to clinician reviewers")
    print("  2. Reviewers fill in 'clinician_label' column")
    print("  3. Calculate agreement rate")
    print("  4. If agreement >= 80%, proceed to Phase D")
    print("  5. If agreement < 80%, refine label logic and re-adjudicate")


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Execute Phase C pipeline."""
    
    print("=" * 80)
    print("PHASE C: COHORT DEFINITION, LABELING, AND POWER ANALYSIS")
    print("=" * 80)
    print(f"Database: {DB_PATH}")
    print("=" * 80)
    
    # Connect to database
    con = duckdb.connect(DB_PATH)
    
    try:
        # C.1: Identify troponin assays
        troponin_items = identify_troponin_assays(con)
        
        # C.2: Define troponin thresholds
        thresholds_df = create_troponin_thresholds(troponin_items)
        
        # C.3: Define MI events
        mi_events = define_mi_events(con, thresholds_df)
        
        # C.4: Define primary labels
        labeled_ecgs = define_primary_labels(con, mi_events)
        
        # C.5: Define control groups
        control_symptomatic, control_asymptomatic = define_control_groups(con)
        
        # C.6: Define comorbidity features
        comorbidity = define_comorbidity_features(con)
        
        # C.9: Create cohort master
        cohort_master = create_cohort_master(
            con, labeled_ecgs, control_symptomatic, control_asymptomatic, comorbidity
        )
        
        # C.8: Power analysis
        decision = power_analysis(cohort_master)
        
        # C.7: Setup label adjudication
        setup_label_adjudication(cohort_master, con)
        
        print("\n" + "=" * 80)
        print("‚úÖ PHASE C COMPLETE")
        print("=" * 80)
        print(f"\nKey Outputs:")
        print(f"  - {OUTPUT_DIR}cohort_master.parquet")
        print(f"  - {OUTPUT_DIR}power_analysis_report.txt")
        print(f"  - {OUTPUT_DIR}adjudication_sample.csv")
        print(f"  - {INTERIM_DIR}troponin_itemids.csv")
        print(f"  - {INTERIM_DIR}troponin_thresholds.csv")
        print(f"  - {INTERIM_DIR}mi_events.csv")
        print(f"\nDecision: {decision}")
        
        if decision == "PROCEED":
            print("\n‚úÖ Ready to proceed to Phase D: Feature Extraction")
        elif decision == "MODIFY":
            print("\n‚ö†Ô∏è  Review recommendations and adjust cohort definition if needed")
        else:
            print("\n‚ùå Dataset insufficient for causal inference")
        
        print("=" * 80)
        
    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    finally:
        con.close()
        print("\nDatabase connection closed.")


if __name__ == "__main__":
    main()
